{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bf2968-6301-4fb3-88a6-6eb49ff1395b",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e101ce-cdcd-4aa6-bd78-b996deb24b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c66cc-de82-4cf0-a2d4-3877e654ab6c",
   "metadata": {},
   "source": [
    "Specify Enron email text files paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71323c0-cbc5-4cf8-a66c-6208ca42cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"./Enron Dataset Tests/enron1\",\n",
    "    \"./Enron Dataset Tests/enron2\",\n",
    "    \"./Enron Dataset Tests/enron3\",\n",
    "    \"./Enron Dataset Tests/enron4\",\n",
    "    \"./Enron Dataset Tests/enron5\",\n",
    "    \"./Enron Dataset Tests/enron6\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a27b18-4fe7-4a81-a241-d5046428f981",
   "metadata": {},
   "source": [
    "Specify function to load emails:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3836f-e79a-44ab-bcf4-6f9d35e0c9f8",
   "metadata": {},
   "source": [
    "After loading the datasets, pre-process them by removing additional white spaces, punctuations, digits, and next lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25962d54-7f7c-4c9e-bde3-c4d02e45bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to E:/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\", download_dir=\"E:/nltk\")\n",
    "\n",
    "def preprocess_email(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25232bef-5d4a-462c-9c7f-ffd1fa358ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def handle_emoji(text):\n",
    "    emoji_corpus = [emoji.demojize(doc) for doc in text]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47bc28da-aafb-4a03-8987-00d041761859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to E:/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../Handlers/\")\n",
    "\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47416981-0550-48a1-86be-24d2bcdd2eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5857\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./enron2\"\n",
    "\n",
    "texts, labels = preprocessing.load_emails(folder_path)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f6b8a6-9740-436e-b096-9e402e116876",
   "metadata": {},
   "source": [
    "Store the enron1 (or any enron folder) data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "427990a2-f772-4945-a4a5-edd729a60fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"label\": labels,\n",
    "    \"text\": texts\n",
    "})\n",
    "\n",
    "match folder_path:\n",
    "    case \"./enron1\":\n",
    "        df.to_csv(\"./enron1.csv\", index=False, escapechar=\"\\\\\")\n",
    "    case \"./enron2\":\n",
    "        df.to_csv(\"./enron2.csv\", index=False, escapechar=\"\\\\\")\n",
    "    case \"./enron3\":\n",
    "        df.to_csv(\"./enron3.csv\", index=False, escapechar=\"\\\\\")\n",
    "    case \"./enron4\":\n",
    "        df.to_csv(\"./enron4.csv\", index=False, escapechar=\"\\\\\")\n",
    "    case \"./enron5\":\n",
    "        df.to_csv(\"./enron5.csv\", index=False, escapechar=\"\\\\\")\n",
    "    case \"./enron6\":\n",
    "        df.to_csv(\"./enron6.csv\", index=False, escapechar=\"\\\\\")\n",
    "    case _:\n",
    "        raise ValueError(\"No path exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f9265-2051-4243-8aea-c9db9ca63851",
   "metadata": {},
   "source": [
    "Extract the urls provided in each email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3efc211d-2c31-483b-81ce-7c2c2fe44725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1687\n"
     ]
    }
   ],
   "source": [
    "email_urls = []\n",
    "\n",
    "for text in texts:\n",
    "    urls = preprocessing.extract_urls(text)\n",
    "    email_urls.extend(urls)\n",
    "\n",
    "print(len(email_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31482d29-328f-4bf9-adf4-3cec87f4fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "stop_words = stopwords.words(\"English\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "X_tfidf = vectorizer.fit_transform(texts)\n",
    "y_tfidf = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b388b7-0083-4489-bad1-9905fe9fb00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      1.00       861\n",
      "        spam       0.99      0.98      0.99       311\n",
      "\n",
      "    accuracy                           0.99      1172\n",
      "   macro avg       0.99      0.99      0.99      1172\n",
      "weighted avg       0.99      0.99      0.99      1172\n",
      "\n",
      "[[859   2]\n",
      " [  5 306]]\n"
     ]
    }
   ],
   "source": [
    "from traintest import SupportVectorMachine\n",
    "\n",
    "svm = SupportVectorMachine(X_tfidf, y_tfidf)\n",
    "svm.train()\n",
    "svm.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0effae66-f27e-4848-a6e5-870d75ebcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "X_count = count_vectorizer.fit_transform(texts)\n",
    "y_count = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fc2b821-9187-4a12-89ba-b69d2d8c621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      0.98      0.97       861\n",
      "        spam       0.95      0.89      0.92       311\n",
      "\n",
      "    accuracy                           0.96      1172\n",
      "   macro avg       0.96      0.94      0.95      1172\n",
      "weighted avg       0.96      0.96      0.96      1172\n",
      "\n",
      "[[847  14]\n",
      " [ 33 278]]\n"
     ]
    }
   ],
   "source": [
    "from traintest import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(X_count.toarray(), y_count)\n",
    "nb.train()\n",
    "nb.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e7eefb4-3a36-4396-b4cd-8bcbedfca56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       861\n",
      "        spam       1.00      0.94      0.97       311\n",
      "\n",
      "    accuracy                           0.98      1172\n",
      "   macro avg       0.99      0.97      0.98      1172\n",
      "weighted avg       0.98      0.98      0.98      1172\n",
      "\n",
      "[[860   1]\n",
      " [ 19 292]]\n"
     ]
    }
   ],
   "source": [
    "from traintest import RandomForest\n",
    "\n",
    "rf = RandomForest(X_tfidf, y_tfidf)\n",
    "rf.train()\n",
    "rf.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ac66b7-8482-4b93-bf6c-6dbb80507793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.98      0.98       861\n",
      "        spam       0.94      0.95      0.94       311\n",
      "\n",
      "    accuracy                           0.97      1172\n",
      "   macro avg       0.96      0.96      0.96      1172\n",
      "weighted avg       0.97      0.97      0.97      1172\n",
      "\n",
      "[[841  20]\n",
      " [ 15 296]]\n"
     ]
    }
   ],
   "source": [
    "from traintest import DecisionTree\n",
    "\n",
    "dt = DecisionTree(X_tfidf, y_tfidf)\n",
    "dt.train()\n",
    "dt.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d877d07-8f89-4a13-87fb-603b351660da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98       861\n",
      "        spam       0.99      0.92      0.95       311\n",
      "\n",
      "    accuracy                           0.98      1172\n",
      "   macro avg       0.98      0.96      0.97      1172\n",
      "weighted avg       0.98      0.98      0.98      1172\n",
      "\n",
      "[[857   4]\n",
      " [ 24 287]]\n"
     ]
    }
   ],
   "source": [
    "from traintest import KNearestNeighbors\n",
    "\n",
    "knn = KNearestNeighbors(X_tfidf, y_tfidf)\n",
    "knn.train()\n",
    "knn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b77614-4131-4e68-b86a-0f3ee7b5bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from traintest import LogisticRegressionModel\n",
    "\n",
    "lr = LogisticRegressionModel(X_tfidf, y_tfidf)\n",
    "lr.train()\n",
    "lr.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
